{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper():\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def test_network(net, trainloader):\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = dataiter.next()\n",
    "\n",
    "    # Create Variables for the inputs and targets\n",
    "    inputs = Variable(images)\n",
    "    targets = Variable(images)\n",
    "\n",
    "    # Clear the gradients from all Variables\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass, then backward pass, then update weights\n",
    "    output = net.forward(inputs)\n",
    "    loss = criterion(output, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def view_recon(img, recon):\n",
    "    ''' Function for displaying an image (as a PyTorch Tensor) and its\n",
    "        reconstruction also a PyTorch Tensor\n",
    "    '''\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
    "    axes[0].imshow(img.numpy().squeeze())\n",
    "    axes[1].imshow(recon.data.numpy().squeeze())\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "        ax.set_adjustable('box-forced')\n",
    "\n",
    "def view_classify(img, ps, version=\"MNIST\"):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    if version == \"MNIST\":\n",
    "        ax2.set_yticklabels(np.arange(10))\n",
    "    elif version == \"Fashion\":\n",
    "        ax2.set_yticklabels(['T-shirt/top',\n",
    "                            'Trouser',\n",
    "                            'Pullover',\n",
    "                            'Dress',\n",
    "                            'Coat',\n",
    "                            'Sandal',\n",
    "                            'Shirt',\n",
    "                            'Sneaker',\n",
    "                            'Bag',\n",
    "                            'Ankle Boot'], size='small');\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBachend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import helper\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,),(0.5,))])\n",
    "\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle = True)\n",
    "\n",
    "\n",
    "testset = datasets.FashionMNIST('~/pytorch/F_MNIST_data/', download = True, train = False, transform = transform)\n",
    "testlaoder = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what one of the images looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17c2e48ac50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the netowrk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
